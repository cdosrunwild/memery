{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "Find the meme you are looking for! <~~ *this one i like*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core module, which loads the CLIP model and the encodings of all the images in the folder, then tokenizes the search text or image, and finally returns a sorted list of filenames.\n",
    "\n",
    "*Express this as a process and nouns, instead of a blob sentence. The next part doesn't count because it's a whole new heading and it's crufted with nerd talk. Say clearly what the core flow is here. Something like:*\n",
    "\n",
    "*Memery takes a folder of images, and a search query, and returns a list of nearest neighbors to the query.*\n",
    "\n",
    "*The query flow and index flow can be used separately, but by default the query flow calls the indexing flow on each search. Image encodings are saved to disk. Only new images will be encoded with each indexing.*\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular flow system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the Neural Search design pattern as described by Han Xiao in e.g. [General Neural Elastic Search and Go Way Beyond](https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond)&c.\n",
    "\n",
    "This is a system designed to be scalable and distributed if necessary. Even for a single-machine scenario, I like the functional style of it: grab data, transform it and pass it downstream, all the way from the folder to the output widget.\n",
    "\n",
    "There are two main types of operater in neural search: **flows** and **executors**.\n",
    "\n",
    "**Flows** are specific patterns of data manipulation and storage. **Executors** are the operators that transform the data within the flow. \n",
    "\n",
    "There are two core flows to any search system: indexing, and querying. The plan here is to make executors that can be composed into flows and then compose the flows into a UI that supports querying and, to some extent, indexing as well.\n",
    "\n",
    "The core executors for this use case are:\n",
    " - Loader\n",
    " - Crafter\n",
    " - Encoder\n",
    " - Indexer\n",
    " - Ranker\n",
    " - Gateway\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gateway Process -- not yet implemented**\n",
    "\n",
    "Takes a query and processes it through either Indexing Flow or Querying Flow, passing along arguments. The main entrypoint for each iteration of the index/query process.\n",
    "\n",
    "Querying Flow can technically process either text or image search, becuase the CLIP encoder will put them into the same embedding space. So we might as well build in a method for either, and make it available to the user, since it's impressive and useful and relatively easy to build. \n",
    "\n",
    "Eventually the Gateway process probably needs to be quite complicated, for serving all the different users and for delivering REST APIs to different clients. We'll need a way to accept text and images as HTTP requests and return JSON dictionaries (especially at the encoder, which will remain server-bound more than any other executor).\n",
    "\n",
    "*Do i actually need any of this? Perhaps if i'm going to implement it myself. But won't i just end up using Jina oonce it gets implemented fully? Or something similar? Well in any case this is meant to be a readable document, so i can explain how this is meant to owrk in the future without being so obtuse that it becomes an obstacle to reading further...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "For now, calling the `queryFlow` process directly is the simplest gateway.\n",
    "\n",
    "*Meaningless. Explain how to use it closer to where it is implemented. Also show an example of it in use. Terrible sentence really.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `index_flow` checks the local directory for files with image extensions, loads the archive, splits out any new images to be encoded and encodes them, then finally builds a treemap and saves it along with the new archive. It returns a tuple with the locations of the archive and treemap.\n",
    "\n",
    "*Split this up into small chunks with code tests. No blob sentences!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from memery.loader import get_image_files, archive_loader, db_loader, treemap_loader \n",
    "from memery.crafter import crafter, preproc\n",
    "from memery.encoder import image_encoder, text_encoder, image_query_encoder\n",
    "from memery.indexer import join_all, build_treemap, save_archives\n",
    "from memery.ranker import ranker, nns_to_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def index_flow(path):\n",
    "    root = Path(path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    filepaths = get_image_files(root)\n",
    "    archive_db = {}\n",
    "    \n",
    "    archive_db, new_files = archive_loader(filepaths, root, device)\n",
    "    print(f\"Loaded {len(archive_db)} encodings\")\n",
    "    print(f\"Encoding {len(new_files)} new images\")\n",
    "\n",
    "#     start_time = time.perf_counter()\n",
    "\n",
    "    crafted_files = crafter(new_files, device)\n",
    "    new_embeddings = image_encoder(crafted_files, device)\n",
    "    \n",
    "    db = join_all(archive_db, new_files, new_embeddings)\n",
    "    print(\"Building treemap\")\n",
    "    t = build_treemap(db)\n",
    "    \n",
    "    print(f\"Saving {len(db)} encodings\")\n",
    "    save_paths = save_archives(root, t, db)\n",
    "#     print(f\"Done in {time.perf_counter() - start_time} seconds\")\n",
    "    \n",
    "    return(save_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the current savefile for testing purposes\n",
    "Path('images/memery.pt').unlink()\n",
    "Path('images/memery.ann').unlink()\n",
    "\n",
    "save_paths = index_flow('./images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('images/memery.pt', 'images/memery.ann')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert save_paths\n",
    "save_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `query_flow` takes a path and a query, checks for an index, loads it and searches through the treemap if it exists, and calls `index_flow` to index it first if it hasn't been.\n",
    "\n",
    "*BLOB!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query_flow(path, query=None, image_query=None):\n",
    "    start_time = time.time()\n",
    "    print('starting timer')\n",
    "    root = Path(path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(\"Checking files\")\n",
    "    dbpath = root/'memery.pt'\n",
    "    db = db_loader(dbpath, device)\n",
    "    treepath = root/'memery.ann'\n",
    "    treemap = treemap_loader(treepath)\n",
    "    \n",
    "    filepaths = get_image_files(root)\n",
    "    if treemap == None or len(db) != len(filepaths):\n",
    "        print('Indexing')\n",
    "        dbpath, treepath = index_flow(root)\n",
    "        treemap = treemap_loader(Path(treepath))\n",
    "        db = db_loader(dbpath, device)\n",
    "    \n",
    "    print('Converting query')\n",
    "    if image_query:\n",
    "        img = preproc(image_query)\n",
    "    if query and image_query:\n",
    "        text_vec = text_encoder(query, device)\n",
    "        image_vec = image_query_encoder(img, device)\n",
    "        query_vec = text_vec + image_vec\n",
    "    elif query:\n",
    "        query_vec = text_encoder(query, device)\n",
    "    elif image_query:\n",
    "        query_vec = image_query_encoder(img, device)\n",
    "    else:\n",
    "        print('No query!')\n",
    "        \n",
    "    print(f\"Searching {len(db)} images\")\n",
    "    indexes = ranker(query_vec, treemap)\n",
    "    ranked_files = nns_to_files(db, indexes)\n",
    "    \n",
    "    print(f\"Done in {time.time() - start_time} seconds\")\n",
    "    \n",
    "    return(ranked_files)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Then what?! What are the limitations of this system? What are its options? What configuration can i do if i'm a power user? Why did you organize things this way instead of a different way?*\n",
    "\n",
    "*This, and probably each of the following notebooks, would benefit from a small recording session where I try to explain it to an imaginary audience. So that I can get the narrative of how it works, and then arrange the code around that.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
